\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% enumerate package lets us use letters instead of numbers
\usepackage{enumerate}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{27.023pt}
\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\sf PLOS}

%% Include all macros below
\newcommand{\fixme}[2]{\textsc{\textbf{{#1}: {#2}}}}
\newcommand{\recommend}[1]{\textit{#1}}
\newcommand{\withurl}[2]{{#1}\footnote{\texttt{#2}}}

\begin{document}
\vspace*{0.2in}


\begin{flushleft}
{\Large
\textbf\newline{Ten Simple Rules for Making Research Software More Robust}
}
\newline
\\
{Morgan~Taschuk}\textsuperscript{1,\ddag *},
{Greg~Wilson}\textsuperscript{2}
\\
\textbf{1} Ontario Institute for Cancer Research / morgan.taschuk@oicr.on.ca
\\
\textbf{2} Software Carpentry Foundation / gvwilson@software-carpentry.org
\\
\bigskip
{\ddag} These authors contributed equally to this work.
\\
* Corresponding author.
\end{flushleft}

\section*{Abstract}

Software produced for research,
published and otherwise,
suffers from a number of common problems
that make it difficult or impossible to run outside the original institution,
or even off the primary developer's computer.
We present ten simple rules to make such software robust enough to run anywhere,
and inspire confidence in your reproducibility,
and thereby delight your users and collaborators.

\section*{Notes}

\begin{itemize}

\item
References:
  \begin{itemize}
  \item https://gigascience.biomedcentral.com/articles/10.1186/2047-217X-2-15
  \item https://gigascience.biomedcentral.com/articles/10.1186/2047-217X-3-31
  \item https://tspace.library.utoronto.ca/handle/1807/73111
  \end{itemize}

\item
Cite http://hmmer.org/ as an example of scientific software done right.

\item
Suggest that people put all the checking and validation up front,
so that if the program is going to crash it does so right away
(rather than in the middle of the night after 18 hours of runtime).

\item
Should we tell people to create several single-purpose tools
rather than One Script to Rule Them All?

\item
Should we tone down the ``tremor of fear'' remark?

\item
``Keep in mind that if a parameter can be adjusted,
users will want to be able to turn off the feature entirely to have a baseline comparison.''
What's the relationship here between parameter and feature?

\item
Honor local conventions,
e.g.,
use \texttt{TMPDIR} for temporary files.
(The problem is,
where are all of these written down?)

\item
Combine versioning and making previous versions available into a single point
in order to free up space for something else to be rule 10?

\end{itemize}

\section*{Author Summary}

\fixme{GW}{Write non-technical paragraph for the general public.}

\linenumbers

\section*{Introduction}

Typically, scientific software is developed and used by a single person,
usually a graduate student or postdoc, and produces intended results in their
hands~\cite{prins2015}. 
But what happens
when someone else wants to use their software? Everyone with
a few years of experience feels a tremor of fear when told to use
a graduated student's code to analyze their data.
Often, that software will be undocumented and work in unexpected
ways (if it works at all).  It will often rely on nonexistent paths or resources,
be tuned for a single dataset,
or simply be an older version than was used in published papers.
The new user is then faced with two unpalatable options:
hack the existing code to make it work, or start over. Being unable to
replicate results is so common that one publication refers to it as "a rite of
passage"~\cite{baker2016}. 

The root cause of this problem is that most research software
isn't \emph{robust}. The difference between running and being
robust is the difference between ``works for me on my machine'' and
``works for other people on a cluster I've never used''. With a lack of
robustness also comes a lack of reproducibility and many duplicated efforts,
slowing the pace of research~\cite{prabhu2011,lawlor2015}.
Bioinformatics is full of such efforts, catalogued by efforts such as
the ELIXIR tools and data registry~\cite{ison2016}
and the Bioinformatics Links Directory~\cite{brazas2012}. In the Bioinformatics
Links Directory as of 2016, there are 84 different multiple sequence aligners, 141 tools
to analyze transcript expression, and 182 pathway and interaction resources.

This problem is not unique to bioinformatics, or even to computing~\cite{baker2016}. 
Best practices in software engineering specifically aim to increase software
robustness. However, most bioinformaticians come from related disciplines and
are trained on-the-job or otherwise informally~\cite{prins2015,atwood2015}. Even
the existing programs and initiatives rarely have the time to cover software engineering
in-depth, especially since the field is so broad and rapidly
developing~\cite{atwood2015,lawlor2015}. Regardless of whether or not the skills
and knowledge are taught, developing robust software is not directly rewarded
in science and funding is difficult to come by~\cite{prins2015}. Some proposed
solutions to this problem, in addition to restructuring educational programs,
include hiring dedicated software engineers~\cite{lawlor2015,sanders2008},
partnering with private sector or grassroots organizations~\cite{prins2015,ison2016},
or using specific technical tools like containerization or cloud
computing~\cite{afgan2016,howe2012}. Each of these requires time and, in some
cases, institutional change.

You don't need to be a full-time programmer to write robust software. In fact,
some of the best, most reliable pieces of software in many scientific
communities are written by researchers~\cite{prabhu2011,sanders2008}. 
The most widely-used software packages tend to adopt strong software
engineering approaches, have high standards of reproducibility, good testing
practices, and inter-institutional collaborations that grow into strong user
bases. In the bioinformatics community, Bioconductor and Galaxy follow this
path~\cite{gentleman2004,afgan2016}.

That said, not \emph{every} coding effort requires such rigor:
as the saying goes, not everything worth doing is worth doing right (or right away).
Code that is used once to answer a specific question related to a specific dataset
doesn't require comprehensive documentation or flexible configuration,
and the only sensible way to test it may well be to run it on the dataset in
question. Exploratory analysis is an iterative process that is developed
quick and revised often~\cite{lawlor2015,sanders2008}. 
However, if a script is dusted off and run three or four
times for slightly different purposes,
is crucial to a publication or a lab,
or being passed on to someone else,
it may be time to make your software more robust.
In particular, robust software:

\begin{itemize}
\item
  works for users other than the original author;
\item
  is kept under version control;
\item
  can be installed on more than one computer or account with relative ease;
\item
  has well-defined input and output formats;
\item
  has documentation that describes what its dependencies are, how to
  install it, and what the options are; and
\item
  comes with enough tests to show that it actually runs.
\end{itemize}

These simple rules are also necessary steps toward creating a reusable
library that can be shared and reused through a site like CPAN or CRAN.  They are
generic and can be applied to all languages, libraries, packages,
documentation styles, and operating systems for both closed-source and
open-source software.  
Whether the aim is as simple as sharing the
code with collaborators or as complex as using the software in a
production analysis environment, increasing the robustness of your
software decreases headaches all around.

\section{Have a README that explains in a few lines what the software does and what its dependencies are.}

The README is the first stop for most new users.
At a minimum, it needs to get a new user started and point them towards more
help, if they need it.
A working example using test data (per
\#9) is always appreciated.

\textbf{Explain what the software does:} At the beginning of the README,
explain what the software does in one or two sentences.
There's nothing more frustrating
than spending the time to download and install some software only to
find out that it doesn't do what you thought it did.

\textbf{List required dependencies:} Often, software depends on very
specific versions of libraries, modules, or operating systems. This is
entirely reasonable as long as it is properly documented. Often,
multiple libraries exist with the same or very similar names, so
either provide the commands necessary to download the dependencies or
link to the software homepage. Include the version number for each
dependency.  

If your dependency is not publicly available,
you have several options to ensure your software remains useable.
You can add plain-text or code directly to
your repository with appropriate attribution. If the dependency is a binary, use a
\withurl{binary repository manager}{https://en.wikipedia.org/wiki/Binary\_repository\_manager}
such as Artifactory or ProGet. These managers keep versioned copies of
software at constant URLs so they can be downloaded as long as the
manager continues to run. If the dependency must remain closed, place it at an
internal location on shared disk, remove all write permissions, and
link to it from your README, although this method is 
discouraged because of the potential for sharing and risks accidental removal.

\textbf{Installation instructions:} If the software needs to be compiled
or installed, list those instructions in the README. Also
mention if you recommend they use a pre-compiled binary instead
through a system such as pip, yum or apt.

\textbf{Input and output files}: All possible input and output files
should be listed in this section. Do the files conform to a particular
standard, an extension of an existing format, or is it your own
format? If using a standard format, link to the specification and
version. If you extend the standard or have your own format, define it
explicitly, listing all the required fields and acceptable values.
(You get bonus points if you include a script to convert between
a standard format and your file format). If there is no rigorous format
(such as with log files), show an example file, or the first few lines,
and explain what the sections mean.

Input files and their formats are included in most documentation, but
intermediate, auxilliary and log files are often missing. 
\emph{All} files should be listed in the README, even those considered self-explanatory. Log
files are often full of valuable information that can be
mined for the user's specific purpose. If your users might need to know,
``Does this program report the percentage of reads trimmed to remove
adapter sequences?'' they should be able to check the README and confidently
say, ``Yes, it is in the log file''.

\textbf{Attributions and licensing:} Attributions are how you credit
your main contributors; licenses are how you want others to use and
credit your software. By putting them in the README, it is readable \emph{before}
the software is installed (or even downloaded).
Leave no
question in anyone's mind about whether your software can be used
commercially, how much modification is permitted, and how other software
needs to credit you. If your software is not open source, state that clearly.
Attributions can also contain a list of ``expert'' users
that can be contacted if new users have problems with the software.

The README file for \withurl{khmer}{https://github.com/dib-lab/khmer/blob/master/README.rst}
is a good model:
it explains the software's purpose,
tells readers where to get help,
and includes links to a CITATION file (explaining how to cite the project)
and the license.

\section{Print usage information when launching from the command line that explains the software's features.}

Usage information provides the first line of help for both first-time and
experienced users of command-line applications.
Ideally, usage is a terse, informative command-line help message that
guides the user in the correct use of your software. Terseness is
important: usage that extends for multiple screens is a nuisance, especially when
printed to standard error instead of standard output (where it can
easily be paged).

Usage should provide all of the information necessary to run the
software. It is invoked either by running the software without
any arguments; running the software with incorrect arguments; or by
explicitly choosing a help or usage option.

An example of good usage is GNU's \texttt{mkdir} command, which makes
new directories:

\begin{small}
\begin{verbatim}
$ mkdir --help
Usage: mkdir [OPTION]... DIRECTORY...
Create the DIRECTORY(ies), if they do not already exist.

Mandatory arguments to long options are mandatory for short options too.
  -m, --mode=MODE   set file mode (as in chmod), not a=rwx - umask
  -p, --parents     no error if existing, make parent directories as needed
  -v, --verbose     print a message for each created directory
  -Z, --context=CTX  set the SELinux security context of each created
                      directory to CTX
      --help     display this help and exit
      --version  output version information and exit

Report mkdir bugs to bug-coreutils@gnu.org
GNU coreutils home page: <http://www.gnu.org/software/coreutils/>
General help using GNU software: <http://www.gnu.org/gethelp/>
For complete documentation, run: info coreutils 'mkdir invocation'
\end{verbatim}
\end{small}

There is no standard format for usage statements, but good ones share
several features:

\textbf{The syntax for running the program}: This includes the name of the
program and defines the relative
location of optional and required flags, arguments and values for execution.
Arguments in {[}square brackets{]}
are usually optional. An ellipsis (\ldots e.g. ``{[}OPTION{]}\ldots{}'')
indicates that more than one value can be provided.

\textbf{Description}: Similar to the README, the
description reminds users of the software's primary function.

\textbf{Most commonly used arguments, a description of each, and the
default values}: Not all arguments need to appear in the usage, but the most
commonly used ones should be listed here. Users will rely on this for
quick reference when working with your software.

\textbf{Where to find more information}: Whether an email address, web
site or manual page, there should be an indication where the user can go
to find more information about the software.

\textbf{Printed to standard output} : So that it can be piped into
\texttt{less}, searched with \texttt{grep}, or compared to the previous
version with \texttt{diff}.

\textbf{Exit with an appropriate exit code}: When usage is invoked by
providing incorrect arguments, the program should exit with a non-zero
code to indicate an error. However, when help is explicitly requested,
the software should not exit with an error.

\section{Give the software a meaningful version number.}

Most software has a version number composed of a decimal number that
increments as new versions are released. There are many different ways
to construct and interpret the version, but most importantly for us, a
particular software version run with the same parameters should give
identical results no matter when it's run. Results include both correct
output as well as any errors.
Every time you release your software, i.e., distribute it to
someone other than yourself and/or the development team, you should
increment your version number.

\withurl{Semantic versioning}{http://semver.org/} is one of the most common
types of versioning for open-source software. Version numbers take the
form of \emph{MAJOR.MINOR{[}.PATCH{]}}, e.g., 0.2.6. The major and
minor numbers versions are almost always provided. Changes in the major
version number herald significant changes in the software that are not
backwards compatible, such as changing or removing features or altering
the primary functions of the software. Increasing the minor version
represents incremental improvements in the software, like adding new
features. Following the minor version number can be an arbitrary number
of project-specific identifiers, including patches, builds and qualifiers.
Common qualifiers include \texttt{alpha}, \texttt{beta}, and \texttt{SNAPSHOT},
for applications that are
not yet stable or released, and \texttt{-RC} for release candidates prior
to official release.

The version of your software should be easily available, both when
supplying \texttt{-\/-version} or \texttt{-v} on the command line as
well as in the results. The software version should be printed to the
same location as all of the other parameters (see Rule 4).  It should
also be included in all of the program's output, particularly debugging
traces: if someone needs help, it's important that they be able to tell
whoever's helping them which version of the software they're using.

\section{Make older versions available.}

Software evolves, but not all software evolves at the same pace.
While more consistent command-line arguments
or more flexible handling of output directories
may make a program better in general,
it can simultaneously make work for someone
who integrated the old version into their own workflow a year or two ago,
and won't see any benefits from upgrading.
A program's authors should therefore ensure that every version they have released
continues to be available.

A number of mechanisms exist for
controlled release that range from as simple as adding an appropriate
commit message or tag to version control, to official releases alongside
code on Sourceforge, Bitbucket or GitHub, to depositing into a
repository like apt, yum, homebrew, CPAN, etc. Choose the method that
best suits the number and expertise of users you anticipate.

The most common way to do this is to include the version number in the
name of the released package, and to link to those old versions from
the project's home page or downloads page.
For example,
as of the time of writing,
the downloads page for the Python programming language includes links to
Versions 3.5.2, 3.5.1, 3.5.0, 3.4.5, 3.4.4, 2.7.12, and 2.7.11,
along with a link to an archive page of even older versions.
Such archives are most useful if these packages document their dependencies,
which we discuss in the next section.

\section{Reuse software (within reason).}

In the spirit of code reuse and interoperability, developers often want
to reuse software written by others. 
With a few lines, a call
is made out to the other program and the results are incorporated into the
primary script. Using popular projects reduces the amount of code that
needs to be maintained and leverages the work done to check that software.

Unfortunately, reusing software introduces dependencies, which can
bring their own special pain.  The interface between two software
packages can also be a source of considerable frustration: all too
often, support requests descend into debugging errors produced by the
other project.

In addition, every package someone has to install
before being able to use yours is a
possible (some would say ``likely'') source of frustration for some
potential user. On the other hand, research software developers should
re-use existing software wherever possible.

To strike a balance between
these two, developers should
\textbf{document \emph{all} of the packages that theirs depends on, preferably in a machine-readable form}.
For example, it is common for Python projects to include a file called
\texttt{requirements.txt} that lists the names of required libraries,
along with version ranges:

\begin{verbatim}
requests>=2.0
pygithub>=1.26,<=1.27
python-social-auth>=0.2.19,<0.3
\end{verbatim}

This file can be read by a package manager, which can check that the
required software is available, and install it if it is not. Similar
mechanisms exist for Perl, R, and other languages.  Whatever is used,
we recommend that developers should \emph{always} install dependencies
using this mechanism, especially on their personal machines, so that
they're sure it works.

Conversely, developers should
\textbf{avoid depending on scripts and tools which are not available as packages},
even (or especially) if they are installed on the computers the original developer is using.
In many cases, a program's author may not realize that some tool was built locally, and
doesn't exist elsewhere. At present, the only sure way to discover such
unknown dependencies is to install on a system administered by someone
else and see what breaks. As use of lightweight
virtualization containers like Docker becomes more widespread, it may
become common to test installation on a virtual machine.

The way the second program is invoked can throw up errors.
The program may not be in the user's path, or it may be an
older or newer version and produce results different than expected.
Windows users will be frustrated if you invoke \texttt{bash} or \texttt{sh} explictly or
use shell-specific conventions like `*' expansion.
Even Linux-standard functions available vary slightly between
installs. For example, GNU \texttt{sort} is available on almost every
*nix distribution, but sorts differently depending on locale.

Despite these caveats,
we fully support the reuse of software between projects provided
three additional guidelines are adhered to. First, 
\textbf{make sure that you really need the auxiliary program}. If you are
executing GNU sort instead of figuring out how to sort lists in Python,
it may not be worth the pain of integration.

Second, \textbf{ensure the appropriate software and version is available}.
Either allow the user to
configure the exact path to the package, distribute the program with the
dependent software, or download it during installation using a
dependency management system. Regardless, check whether the program is
executable and what version is running. Be sure to remind users in the
documentation what the compatible versions are.

Finally, to ensure support on as many different operating systems as
possible, \textbf{use native functions for starting other processes}, such as
Java's \texttt{Runtime.exec} call, Python's subprocess module, and Perl's system
command, and be sure to capture and report the output of the subprocess's standard error
to facilitate debugging.

\section{Do not require root or other special privileges.}

Root (also known as ``superuser'' or ``admin'') is a special account on
a computer that has (among other things) the power to modify or delete
system files and user accounts. Conversely, files and directories owned
by root usually cannot be modifed by normal users.

Installing or running a program with root privileges is often
convenient, since doing so automatically bypasses all those pesky safety
checks that might otherwise get in the user's way. However, those checks
are there for a reason: scientific software packages may not
intentionally be malware, but one small bug or over-eager file-matching
expression can certainly make them behave as if they were. Outside of
very unusual circumstances,
\textbf{packages should not require root privileges to set up or use}.

Another reason for this rule is that users may want to try out a new
package before installing it system-wide on a cluster. Requiring root
privileges will frustrate such efforts, and thereby reduce uptake of the
package. Requiring that software be installed under its own user account
(e.g., that \texttt{packagename} be made a user, and all of the
package's software be installed in that ``user's'' space) is similarly
limiting, and makes side-by-side installation of multiple versions of
the package more difficult.

Developers should therefore
\textbf{allow packages to be installed in an arbitrary location},
e.g., under a user's home directory in
\texttt{\textasciitilde{}/packagename}, or in directories with standard
names like \texttt{bin}, \texttt{lib}, and \texttt{man} under a chosen
directory. If the first option is chosen, the user may need to modify
her search path to include the package's executables and libraries, but
this can (more or less) be automated, and is much less risky than
setting things up as root.

Note:
testing software installation has traditionally been regarded as difficult,
since it necessarily alters the machine on which the test is conducted.
Lightweight virtualization containers like Docker make this much easier as well.

\section{Eliminate hard-coded paths.}

It's easy to write software that reads input from a file called
\texttt{mydata.csv}, but also very limiting. If a colleague asks you to
process her data, you must either overwrite your data file (which is
risky) or edit your code to read \texttt{otherdata.csv} (which is also
risky, because there's every likelihood you'll forget to change the
filename back, or will change three uses of the filename but not a
fourth).

Hard-coding file paths in a program also makes the software harder to run
in other environments. If your package is installed on a cluster, for
example, the user's data will almost certainly \emph{not} be in the same
directory as the software, and the folder
\texttt{C:\textbackslash{}users\textbackslash{}yourname\textbackslash{}}
will probably not even exist.

For these reasons, users should be able to
\textbf{set the names and locations of input and output files as command-line parameters}.
This rule applies to reference data sets as well as the user's own
data: if a user wants to try a new gene identification algorithm using
a different set of genes as a training set, she should not have to
edit the software to do so.

A corollary to this rule is
\textbf{do not require users to navigate to a particular directory to do their work}.
``Where I have to be'' is just another hard-coded path.

In order to save typing, it is often convenient to allow users to
specify an input or output \emph{directory}, and then require that there
be files with particular names in that directory. This practice, which
is sometimes called ``convention over configuration'', is used by many
software frameworks, such as WordPress and Ruby on Rails, and often
strikes a good balance between adaptability and consistency.

\section{Allow configuration of all useful parameters from the command line.}

Useful parameters are those values that a user will need to modify to suit
their computer, dataset or application. 
The full list of useful parameters is software-specific and so cannot be
detailed here, but a short list includes
input and reference files and directories,
output files and directories,
filtering,
tuning,
random number generation seeds,
and
any alternatives that you've built-in,
e.g.,
compress results, use a different calculation, verbose output, etc.

Providing parameters on the
command line increases the flexibility and usability of the program. You
may have determined early in development that 0.58 is an optimal seed for your
original dataset, but that doesn't mean that is the best seed for every
case. Being able to change parameters on the fly to determine if and how
they change the results is important as your software gains more users,
facilitating exploratory analysis and parameter sweeping. Keep in mind
that if a parameter can be adjusted, users will want to be able to turn
off the feature entirely to have a baseline comparison.

When the software starts, it should echo all parameters and software
versions to standard out or a log file alongside the results. This
feature supports greater reproducibility because any result can be
replicated with only the previous output files as reference.

You can set reasonable default values
as long as any command line arguments
override those values. Configuration files should be used in preference
to hardcoding the defaults directly. Configuration files can be in a standard
location, e.g. \texttt{.packagerc} in the user's home directory or
provided on the command line as an additional argument.

We caution against overusing configuration files. When a user needs to
locate, open, change and save a file in order to change a parameter, the
import of the change seems larger and discourages experimentation. 
Only values that are unlikely to
change between runs belong in the config file, such as dependencies,
servers, version numbers, network drives, and any other defaults for
your lab or institution. 
Since
all parameters should be echoed in the results, config files should be
cleaned up after execution completes or they occupy valuable disk space.
Use a default configuration file and specify all other parameter values on the
command line.

Finally,
if the software is configurable in any way,
it should \textbf{check that all input values are in a reasonable range},
\textbf{choose reasonable defaults where they exist},
and \textbf{set no defaults at all when there aren't any reasonable ones}.
Software should also \textbf{check that configuration values are reasonable at startup}:
few things are as annoying as having a program announce after running for two hours
that it isn't going to save its results
because the requested directory doesn't exist.

\section{Include a small test set that can be run to ensure the software is actually working.}

Every package should come with a small test script for users to run
after installation. Its purpose is \emph{not} to check that the software
is working correctly (although that is extremely helpful), but rather to
ensure that it will work at all. This test script can also serve as a
working example of how to run the software.

In order to be useful, this test script must be easy to find and run. A
single file in the project's root directory named \texttt{runtests.sh}
or something equally obvious is a much better solution than documenting
test cases and requiring people to copy and paste them into the shell.

Equally, the test script's output must be easy to interpret. Screens
full of correlation coefficients do not qualify: instead, the script's
output should be simple to understand for non-experts,
such as one line per test, with the test's name
and its pass/fail status, followed by a single summary line saying how
many tests were run and how many passed or failed. If many or all tests
fail because of missing dependencies, that fact should be displayed
once, clearly, rather than once per test, so that users have a clear
idea of what they need to fix and how much work it's likely to take.

Finally,
studies have found that the ease with which people can start making
contributions is a strong predictor of whether they will or not~\cite{steinmacher2015}.
By making it simpler for outsiders to contribute,
a test suite of any kind also makes it more likely that they will.

\section{Produce identical results when given identical inputs.}

Given a set of parameters and a dataset, a particular version of a program
should produce the same results every time it is run
to aid testing, debugging, and reproducibility.
Even minor changes to code can cause minor changes in output because of floating-point issues,
which means that getting exactly the same output for the same input and parameters
probably won't work during development,
but it should still be a goal for people who have deployed a specific version.

Many applications rely on randomized algorithms to
improve performance or runtimes. As a consequence, results can change
between runs, even when provided with the same data and parameters. By
its nature, this randomness renders strict reproducibility impossible.
Debugging is more difficult. If even the small test set (\#9) produces
different results, new users may not be able to tell whether the software is
working properly. When comparing
results between versions or after changing parameters, even small
differences can confuse or muddy the comparison. And especially when
producing results for publications, grants or diagnoses, any analysis
should be absolutely reproducible.

Given the size of biological data, it is unreasonable to suggest that
random algorithms be removed. However, most programs use a pseudo-random
number generator, which uses a starting seed and an equation to
approximate random numbers. Setting the seed to a consistent value
removes randomness between runs. Allow the user to optionally provide
the seed as an input parameter, thus rendering the program deterministic
for those cases where it matters. If the seed is set internally (e.g.,
using clock time), echo it to the output for re-use later.

\section*{What We Left Out}

Any short list of rules for developing robust software will
necessarily be incomplete.  A few of our more significant omissions
include:

\begin{description}

\item[\textbf{Services}] The rules we present above are necessary, but
  not sufficient, for software that needs to interact with services
  such as database and web servers.  Such software is often stopped
  and started under automatic control, may need to authenticate, etc.,
  all of which are out of scope of this paper.

\item[\textbf{Documentation}] We think it is very important for
  developers to document their work, but our experience is that people
  are unlikely do it during normal development, and we don't want to
  recommend practices that people won't actually adopt.

\item[\textbf{Build Systems}] If the software depends on a build tool,
  either because it is written in a compiled language or to manage
  workflow, it should use a standard tool such as Make rather than
  home-brewed scripts.

\end{description}

\section*{Conclusion}

There has been extended discussion over the past few years of the
sustainability of research software, but this question is meaningless
in isolation: any piece of software can be sustained if its users are
willing to put in enough effort.  The real equation is the ratio
between the skill and effort available, and the ease with which
software can be installed, understood, used, maintained, and extended.
Following the ten rules we outline here reduce the denominator, and
thereby enable researchers to build on each other's work more easily.

\bibliography{robust-software}

\end{document}
