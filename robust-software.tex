\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% enumerate package lets us use letters instead of numbers
\usepackage{enumerate}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}

% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{27.023pt}
\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\sf PLOS}

%% Include all macros below
\newcommand{\fixme}[2]{\textsc{\textbf{{#1}: {#2}}}}
\newcommand{\recommend}[1]{\textit{#1}}
\newcommand{\withurl}[2]{{#1}\footnote{\texttt{#2}}}

\begin{document}
\vspace*{0.2in}

\begin{flushleft}
{\Large
\textbf\newline{Ten Simple Rules for Making Research Software More Robust}
}
\newline
\\
{Morgan~Taschuk}\textsuperscript{1,\ddag *},
{Greg~Wilson}\textsuperscript{2,\ddag}
\\
\textbf{1} Ontario Institute for Cancer Research / morgan.taschuk@oicr.on.ca
\\
\textbf{2} Software Carpentry Foundation / gvwilson@software-carpentry.org
\\
\bigskip
{\ddag} These authors contributed equally to this work.
\\
* Corresponding author.
\end{flushleft}

\section*{Abstract}

Software produced for research,
published and otherwise,
suffers from a number of common problems
that make it difficult or impossible to run outside the original institution,
or even off the primary developer's computer.
We present ten simple rules to make such software robust enough to run anywhere,
and inspire confidence in your reproducibility,
and thereby delight your users and collaborators.

\section*{Author Summary}

Many researchers have found out the hard way that there's a world of difference
between ``works for me on my machine'' and ``works for other people on theirs''.
Many common challenges can be avoided by following a few simple rules; doing so
not only accelerates research, but also improves reproducibility.

\linenumbers

\section*{Introduction}

Scientific software is typically developed and used by a single person,
usually a graduate student or postdoc~\cite{prins2015}.
It may produce the intended results in their hands,
but what happens when someone else wants to run it? Everyone with
a few years of experience feels a bit nervous when told to use
another person's code to analyze their data:
it will often be undocumented,
work in unexpected ways (if it works at all),
rely on nonexistent paths or resources,
be tuned for a single dataset,
or simply be an older version than was used in published papers.
The potential new user is then faced with two unpalatable options:
hack the existing code to make it work, or start over.

Being unable to
replicate results is so common that one publication refers to it as "a rite of
passage"~\cite{baker2016}. 
The root cause of this problem is that most research software
isn't \emph{robust}. The difference between running and being
robust is the difference between ``works for me on my machine'' and
``works for other people on a cluster I've never used''. The lack of
robustness also leads to many duplicated efforts,
which slows the pace of research~\cite{prabhu2011,lawlor2015}.
Bioinformatics software repositories \cite{ison2016,brazas2012} catalogue dozens to
hundreds of tools to perform the same task:
for example,
in 2016 the Bioinformatics Links Directory included 84 different multiple sequence aligners, 141 tools
to analyze transcript expression, and 182 pathway and interaction resources.
Some of these tools are legitimate efforts to improve the state-of-the-art, but
often they are difficult to install and run~\cite{stajich2002,Seemann2013}, and are effectively abandoned
after publication~\cite{nekrutenko2012}.

This problem is not unique to bioinformatics, or even to computing~\cite{baker2016}. 
Best practices in software engineering specifically aim to increase software
robustness. However, most bioinformaticians learn what they know about software development
on the job or otherwise informally~\cite{prins2015,atwood2015}.
Existing training programs and initiatives rarely have the time to cover software engineering
in depth, especially since the field is so broad and developing so rapidly~\cite{atwood2015,lawlor2015}.
In addition, making software robust is not directly rewarded
in science, and funding is difficult to come by~\cite{prins2015}. Some proposed
solutions to this problem include restructuring educational programs,
hiring dedicated software engineers~\cite{lawlor2015,sanders2008},
partnering with private sector or grassroots organizations~\cite{prins2015,ison2016},
or using specific technical tools like containerization or cloud
computing~\cite{afgan2016,howe2012}. Each of these requires time and, in some
cases, institutional change.

The good news is,
you don't need to be a professionally-trained programmer to write robust software. In fact,
some of the best, most reliable pieces of software in many scientific
communities are written by researchers~\cite{prabhu2011,sanders2008}
who have adopted strong software
engineering approaches, have high standards of reproducibility, use good testing
practices, and foster inter-institutional collaborations that grow into strong user
bases. In the bioinformatics community, Bioconductor and Galaxy follow this
path~\cite{gentleman2004,afgan2016}.

So what \emph{is} ``robust'' software?
In brief, it:

\begin{itemize}
\item
  works for users other than the original author;
\item
  is kept under version control;
\item
  can be installed on more than one computer or account with relative ease;
\item
  has well-defined input and output formats;
\item
  has documentation that describes what its dependencies are, how to
  install it, and what the options are; and
\item
  comes with enough tests to show that it actually runs.
\end{itemize}

These simple rules don't just ensure that the software will run for other people;
they are also necessary steps toward creating a reusable
library that can be shared and reused through a site like CPAN or CRAN.  They are
generic and can be applied to all languages, libraries, packages,
documentation styles, and operating systems for both closed-source and
open-source software.  
Whether the aim is as simple as sharing the
code with collaborators or as complex as using the software in a
production analysis environment, increasing the robustness of your
software decreases headaches all around.

\section{Have a README that briefly explains what the software does and what its dependencies are.}

The README is the first stop for most new users.
At a minimum, it needs to get a new user started and point them towards more
help, if they need it. 
Numerous guidelines exist on how to write good
READMEs\cite{Johnson1997,gnustandards};
their key common features are listed below.

\textbf{Explain what the software does:} At the beginning of the README,
explain what the software does in one or two sentences.
There's nothing more frustrating
than downloading and installing something only to
find out that it doesn't do what you thought it did.

\textbf{List required dependencies:} Often, software depends on
specific versions of libraries, modules, or operating systems. 
This is
entirely reasonable as long as it is properly documented. Include the name and
version number for each dependency.  Several methods exist for acquiring
required packages: either provide the commands necessary to download the
dependencies; link to the software homepage; or (our preference) use a
dependency manager. We address dependencies in more detail in rule 5.

\textbf{Provide compilation/installation instructions:}
If the software needs to be compiled
or installed, list those instructions in the README. Also
mention if you recommend they use a pre-compiled binary instead
through a system such as pip, yum, or apt.

\textbf{List input and output files}: All possible input and output files
should be listed in this section. Do the files use a particular
standard, an extension of an existing format, or your own
format?  If they use a standard format, link to the specification and
version. If you extend the standard or have your own format, define it
explicitly, listing all the required fields and acceptable values.
(You get bonus points if you include a script to convert between
a standard format and your file format). If there is no rigorous format
(which is common with log files), show at least a few lines from an
example file and explain what the sections mean.

Input files and their formats are included in most documentation, but
intermediate, auxiliary, and log files are often missing.
\emph{All} files should be listed in the README, even those considered self-explanatory. Log
files are often full of valuable information that can be
mined for the user's specific purpose. If your users might need to know,
``Does this program report the percentage of reads trimmed to remove
adapter sequences?'' they should be able to check the README and confidently
say, ``Yes, it is in the log file''.

\textbf{State attributions and licensing:} Attributions are how you credit
your main contributors; licenses are how others may use and
credit your work. These files should be available \emph{before}
the software is installed (or even downloaded).
Leave no
question in anyone's mind about whether your software can be used
commercially, how much modification is permitted, and how other software
needs to credit you. If your software is not open source, state that clearly.
Attributions can also contain a list of ``expert'' users
that can be contacted if new users have problems with the software.

The README file for \withurl{khmer}{https://github.com/dib-lab/khmer/blob/master/README.rst}
is a good model:
it explains the software's purpose,
tells readers where to get help,
and includes links to a CITATION file (explaining how to cite the project)
and the license file.

\section{Tell the user what could be done and what was actually done.}

A robust program should
\textbf{print usage information when launching from the command line that explains the software's features}.
Usage information provides the first line of help for both first-time and
experienced users of command-line applications.
Ideally, usage is a terse, informative command-line help message that
guides the user in the correct use of the software. Terseness is
important: usage that extends for multiple screens is a nuisance, especially when
printed to standard error instead of standard output (where it can
easily be paged).  Long output is also much less likely to be read{\ldots}

Usage should provide all of the information necessary to run the
software. It is invoked either by running the software without
any arguments, running it with incorrect arguments, or by
explicitly choosing a help or usage option.
More standard command-line behaviours are detailed in \cite{Seemann2013}.

Almost all command-line applications use a combination of
POSIX~\cite{posix2016} and GNU~\cite{gnustandards} standards for usage. 
An example of good usage is GNU's \texttt{mkdir} command, which makes
new directories:

\begin{small}
\begin{verbatim}
$ mkdir --help
Usage: mkdir [OPTION]... DIRECTORY...
Create the DIRECTORY(ies), if they do not already exist.

Mandatory arguments to long options are mandatory for short options too.
  -m, --mode=MODE   set file mode (as in chmod), not a=rwx - umask
  -p, --parents     no error if existing, make parent directories as needed
  -v, --verbose     print a message for each created directory
  -Z, --context=CTX  set the SELinux security context of each created
                      directory to CTX
      --help     display this help and exit
      --version  output version information and exit

Report mkdir bugs to bug-coreutils@gnu.org
GNU coreutils home page: <http://www.gnu.org/software/coreutils/>
General help using GNU software: <http://www.gnu.org/gethelp/>
For complete documentation, run: info coreutils 'mkdir invocation'
\end{verbatim}
\end{small}

The key features of this usage message are:

\begin{itemize}

\item[\textbf{The syntax for running the program}] This includes the
  name of the program and defines the relative location of optional
  and required flags, arguments and values for execution.  Arguments
  in {[}square brackets{]} are usually optional. An ellipsis (\ldots
  e.g. ``{[}OPTION{]}\ldots{}'') indicates that more than one value
  can be provided.

\item[\textbf{Description}] Similar to the README, the description
  reminds users of the software's primary function.

\item[\textbf{Most commonly used arguments, a description of each, and
    the default values}] Not all arguments need to appear in the
  usage, but those most commonly used should be listed. Users will
  rely on this for quick reference when working with the software.

\item[\textbf{Where to find more information}] Whether it's an email
  address, web site, or manual page, there should be an indication
  where the user can go to find out more about the software.

\item[\textbf{Printed to standard output}] Usage should be displayed
  on standard output so that it can be piped into \texttt{less},
  searched with \texttt{grep}, or compared to the previous version
  with \texttt{diff}.

\item[\textbf{Exit with an appropriate exit code}] When usage is
  invoked by providing incorrect arguments, the program should exit
  with a non-zero code to indicate an error. However, when help is
  explicitly requested, the software should not exit with an error.

\end{itemize}

The usage message tells users what the program could do.
It is equally important for the program to tell users what it actually did.
Accordingly,
when the program starts, it should \textbf{echo all parameters and software
versions to standard out or a log file alongside the results}. This
feature supports greater reproducibility because any result can be
replicated with only the previous output files as reference.

(And as a corollary
if the software is configurable in any way,
it should \textbf{check that all input values are in a reasonable range at startup}. 
Few things are as annoying as having a program announce after running for two hours
that it isn't going to save its results
because the requested directory doesn't exist.)

\section{Make common operations easy to control.}

Being able to change parameters on the fly to determine if and how
they change the results is important as your software gains more users,
as it facilitates exploratory analysis and parameter sweeping.
Programs should therefore
\textbf{allow the most commonly changed parameters to be configured from the command line}.

Users will want to change some values more often than others.
Which ones is software-specific and so cannot be detailed here,
but a short list includes input and reference files and directories,
output files and directories,
filtering parameters,
random number generation seeds,
and
alternatives such as compressing results,
use a variant algorithm,
or verbose output.

To make programs even easier to use,
\textbf{choose reasonable defaults where they exist}
and \textbf{set no defaults at all when there aren't any reasonable ones}.
You can set reasonable default values
as long as any command line arguments
override those values.

Changeable values should \emph{never} be hard-coded:
if users have to edit your software in order to run it,
you have done something wrong.
Changeable but infrequently-changed values should therefore be stored in configuration files.
These can be in a standard location,
e.g. \texttt{.packagerc} in the user's home directory,
or provided on the command line as an additional argument.
Configuration files are often created during installation
to set up such things as server names,
network drives,
and other defaults for your lab or institution. 

\section{Version your releases.}

Software evolves over time, with developers adding or removing features as need
dictates. Making official releases stamps a particular set of features with a
project-specific identifier so that version can be retrieved for later use. For
example, if a paper is published, the software should be released at the same
time so that the results can be reproduced. 

Most software has a version number composed of a decimal number that
increments as new versions are released. 
There are many different ways
to construct and interpret this number, but most importantly for us, a
particular software version run with the same parameters should give
identical results no matter when it's run. Results include both correct
output as well as any errors.
\textbf{Increment your version number every time you release your software to
other people}.

\withurl{Semantic versioning}{http://semver.org/} is one of the most common
types of versioning for open-source software. Version numbers take the
form of \emph{MAJOR.MINOR{[}.PATCH{]}}, e.g., 0.2.6.
Changes in the major
version number herald significant changes in the software that are not
backwards compatible, such as changing or removing features or altering
the primary functions of the software. Increasing the minor version
represents incremental improvements in the software, like adding new
features. Following the minor version number can be an arbitrary number
of project-specific identifiers, including patches, builds and qualifiers.
Common qualifiers include \texttt{alpha}, \texttt{beta}, and \texttt{SNAPSHOT},
for applications that are
not yet stable or released, and \texttt{-RC} for release candidates prior
to an official release.

\textbf{The version of your software should be easily available by 
supplying \texttt{-\/-version} or \texttt{-v} on the command line}. This command should print
the software name and version number, and it should
also be \textbf{included in all of the program's output}, particularly debugging
traces.  If someone needs help, it's important that they be able to tell
whoever's helping them which version of the software they're using.

While new releases may make a program better in general,
they can simultaneously create work for someone
who integrated the old version into their own workflow a year or two ago,
and won't see any benefits from upgrading.
A program's authors should therefore \textbf{ensure that old released versions
continue to be available.}
A number of mechanisms exist for
controlled release that range from as simple as adding an appropriate
commit message or tag to version control, to official releases alongside
code on Bitbucket or GitHub, to depositing into a
repository like apt, yum, homebrew, CPAN, etc. Choose the method that
best suits the number and expertise of users you anticipate.

\section{Reuse software (within reason).}

In the spirit of code reuse and interoperability, developers often want
to reuse software written by others. 
With a few lines, a call
is made out to the other program and the results are incorporated into the
primary script. Using popular projects reduces the amount of code that
needs to be maintained and leverages the work done by the other software.

Unfortunately, reusing software introduces dependencies, which can
bring their own special pain.  The interface between two software
packages can also be a source of considerable frustration: all too
often, support requests descend into debugging errors produced by the
other project. 
In addition, every package someone has to install
before being able to use yours is a
possible (some would say ``likely'') source of frustration for some
potential user. The way the second program is invoked can throw up errors.
The program may not be in the user's path, or it may be an
older or newer version and produce results different than expected.
Windows users will be frustrated if you invoke \texttt{bash} or \texttt{sh} explicitly or
use shell-specific conventions like `*' expansion.
Even Linux-standard functions available vary slightly between
installs. For example, GNU \texttt{sort} is available on almost every
*nix distribution, but sorts differently depending on locale.

Despite these problems, software developers in research should
re-use existing software provided three guidelines are adhered to.
First, 
\textbf{make sure that you really need the auxiliary program}. If you are
executing GNU sort instead of figuring out how to sort lists in Python,
it may not be worth the pain of integration.

Second, \textbf{ensure the appropriate software and version is available}.
Early in execution, check whether the dependency is executable and whether the
version is compatible.
Either allow the user to
configure the exact path to the package, distribute the program with the
dependent software, or download it during installation using a
dependency management system (see Rule 5). 

Finally, to ensure support on as many different operating systems as
possible, \textbf{use native functions for starting other processes}, such as
Java's \texttt{Runtime.exec} call, Python's \texttt{subprocess} module, and Perl's \texttt{system}
command, and be sure to capture and report the output of the subprocess's standard error
to facilitate debugging.

\section{Rely on build tools and package managers for installation.}

Programmers routinely use build tools like Make, Rake, Maven, Ant or MS Build
to compile code, deploy applications, and automate other tasks.
These tools can also be used to manage runtime environments,
i.e.,
to check that the right versions of required packages are installed
and install or upgrade them if they are not.

The same tools can and should be used to manage runtime environments on users' machines as well.
Accordingly,
developed should
\textbf{document all dependencies in a machine-readable form}.
Package managers like apt and yum are available on most Unix-like systems, and
application package managers exist for specific languages like Python (pip),
Java (Maven/Gradle), and Ruby (RubyGems). These package managers can be used
together with the build utility to ensure that dependencies are available at
compile/run time.

For example, it is common for Python projects to include a file called
\texttt{requirements.txt} that lists the names of required libraries,
along with version ranges:

\begin{verbatim}
requests>=2.0
pygithub>=1.26,<=1.27
python-social-auth>=0.2.19,<0.3
\end{verbatim}

This file can be read by the pip package manager, which can check that the
required software is available and install it if it is not. 
Whatever is used,
developers should \emph{always} install dependencies
using their dependency description, especially on their personal machines, so that
they're sure it works.

Conversely, developers should
\textbf{avoid depending on scripts and tools which are not available as packages}.
In many cases, a program's author may not realize that some tool was built locally, and
doesn't exist elsewhere. At present, the only sure way to discover such
unknown dependencies is to install on a system administered by someone
else and see what breaks. As use of virtualization containers becomes more
widespread, software installation can also be tested on a virtual machine or
container system like Docker.

\section{Do not require root or other special privileges.}

Root (also known as ``superuser'' or ``admin'') is a special account on
a computer that has (among other things) the power to modify or delete
system files and user accounts. Conversely, files and directories owned
by root usually cannot be modifed by normal users.

Installing or running a program with root privileges is often
convenient, since doing so automatically bypasses all those pesky safety
checks that might otherwise get in the user's way. However, those checks
are there for a reason: scientific software packages may not
intentionally be malware, but one small bug or over-eager file-matching
expression can certainly make them behave as if they were. Outside of
very unusual circumstances,
\textbf{packages should not require root privileges to set up or use}.

Another reason for this rule is that users may want to try out a new
package before installing it system-wide on a cluster. Requiring root
privileges will frustrate such efforts, and thereby reduce uptake of the
package. Requiring, as Apache Tomcat does, that software be installed
under its own user account---i.e.,
that \texttt{packagename} be made a user, and all of the
package's software be installed in that pseudo-user's space---is similarly limiting,
and makes side-by-side installation of multiple versions of
the package more difficult.

Developers should therefore
\textbf{allow packages to be installed in an arbitrary location},
e.g., under a user's home directory in
\texttt{\textasciitilde{}/packagename}, or in directories with standard
names like \texttt{bin}, \texttt{lib}, and \texttt{man} under a chosen
directory. If the first option is chosen, the user may need to modify
her search path to include the package's executables and libraries, but
this can (more or less) be automated, and is much less risky than
setting things up as root.

Testing the ability to install software has traditionally been regarded as difficult,
since it necessarily alters the machine on which the test is conducted.
Lightweight virtualization containers like Docker make this much easier as well, 
or simply \textbf{ask another person to try and build your software}.

\section{Eliminate hard-coded paths.}

It's easy to write software that reads input from a file called
\texttt{mydata.csv}, but also very limiting. If a colleague asks you to
process her data, you must either overwrite your data file (which is
risky) or edit your code to read \texttt{otherdata.csv} (which is also
risky, because there's every likelihood you'll forget to change the
filename back, or will change three uses of the filename but not a
fourth).

Hard-coding file paths in a program also makes the software harder to run
in other environments. If your package is installed on a cluster, for
example, the user's data will almost certainly \emph{not} be in the same
directory as the software, and the folder
\texttt{C:\textbackslash{}users\textbackslash{}yourname\textbackslash{}}
will probably not even exist.

For these reasons, users should be able to
\textbf{set the names and locations of input and output files as command-line parameters}.
This rule applies to reference data sets as well as the user's own
data: if a user wants to try a new gene identification algorithm using
a different set of genes as a training set, she should not have to
edit the software to do so.
A corollary to this rule is
\textbf{do not require users to navigate to a particular directory to do their work},
since ``where I have to be'' is just another hard-coded path.

In order to save typing, it is often convenient to allow users to
specify an input or output \emph{directory}, and then require that there
be files with particular names in that directory. This practice, which
is sometimes called ``convention over configuration'', is used by many
software frameworks, such as WordPress and Ruby on Rails, and often
strikes a good balance between adaptability and consistency.

\section{Include a small test set that can be run to ensure the software is actually working.}

Every package should come with a small test script for users to run
after installation. Its purpose is not only to check that the software
is working correctly (although that is extremely helpful), but also to
ensure that it works at all. This test script can also serve as a
working example of how to run the software.

In order to be useful, \textbf{make the test script easy to find and run}. 
Many build systems will also run unit tests if provided them at compile time.
If the build system is not amenable to testing, a 
single file in the project's root directory named \texttt{runtests.sh}
or something equally obvious is a much better solution than documenting
test cases and requiring people to copy and paste them.

Equally, \textbf{make the test script's output easy to interpret}. Screens
full of correlation coefficients do not qualify: instead, the script's
output should be simple to understand for non-experts,
such as one line per test, with the test's name
and its pass/fail status, followed by a single summary line saying how
many tests were run and how many passed or failed. If many or all tests
fail because of missing dependencies, that fact should be displayed
once, clearly, rather than once per test, so that users have a clear
idea of what they need to fix and how much work it's likely to take.

Research has shown that the ease with which people can start making
contributions is a strong predictor of whether they will or not~\cite{steinmacher2015}.
By making it simpler for outsiders to contribute,
a test suite of any kind also makes it more likely that they will, and software
with collaborators stands a better chance of surviving in the busy field of
scientific software.

\section{Produce identical results when given identical inputs.}

Given a set of parameters and a dataset, \textbf{a particular version of a program
should produce the same results every time it is run}
to aid testing, debugging, and reproducibility.
Even minor changes to code can cause minor changes in output because of floating-point issues,
which means that getting exactly the same output for the same input and parameters
probably won't work during development,
but it should still be a goal for people who have deployed a specific version.

Many applications rely on randomized algorithms to
improve performance or runtimes. As a consequence, results can change
between runs, even when provided with the same data and parameters. By
its nature, this randomness renders strict reproducibility and therefore
debugging more difficult. If even the small test set (\#9) produces
different results for each run, new users may not be able to tell whether the software is
working properly. When comparing
results between versions or after changing parameters, even small
differences can confuse or muddy the comparison. And especially when
producing results for publications, grants or diagnoses, any analysis
should be absolutely reproducible.

Given the size of biological data, it is unreasonable to suggest that
random algorithms be removed. However, most programs use a pseudo-random
number generator, which uses a starting seed and an equation to
approximate random numbers. Setting the seed to a consistent value
can remove randomness between runs. \textbf{Allow the user to optionally provide
the random seed as an input parameter}, thus rendering the program deterministic
for those cases where it matters. If the seed is set internally (e.g.,
using clock time), echo it to the output for re-use later. 
If setting the seed is not possible, \textbf{make sure the acceptable tolerance is
known and detailed in documentation and in the tests}.

\section*{What We Left Out}

Any short list of rules for developing robust software will
necessarily be incomplete.   Our more significant omissions
include:

\begin{description}

\item[\textbf{Documentation}] We think it is very important for
  developers to document their work, but our experience is that people
  are unlikely do it during normal development, and we don't want to
  recommend practices that people won't actually adopt. However, it is worth
  noting that software that is widely used and contributed to has and enforces
  the need for good documentation~\cite{gentleman2004}.
  Numerous resources exist for writing high quality
  documentation~\cite{karimzadeh2016} and so we do not cover it here.

\item[\textbf{Services}] The rules we present above are necessary, but
  not sufficient, for software that needs to interact with services
  such as database and web servers.  Such software is often stopped
  and started under automatic control, may need to authenticate, etc.,
  all of which are out of scope of this paper.

\end{description}

\section*{Conclusion}

There has been extended discussion over the past few years of the
sustainability of research software, but this question is meaningless
in isolation: any piece of software can be sustained if its users are
willing to put in enough effort.  The real equation is the ratio
between the skill and effort available, and the ease with which
software can be installed, understood, used, maintained, and extended.
Following the ten rules we outline here reduce the denominator, and
thereby enable researchers to build on each other's work more easily.

That said, not \emph{every} coding effort needs to be engineered to
last.  Code that is used once to answer a specific question related to
a specific dataset doesn't require comprehensive documentation or
flexible configuration, and the only sensible way to test it may well
be to run it on the dataset in question. Exploratory analysis is an
iterative process that is developed quick and revised
often~\cite{lawlor2015,sanders2008}.  However, if a script is dusted
off and run three or four times for slightly different purposes, is
crucial to a publication or a lab, or being passed on to someone else,
it may be time to make your software more robust.

\bibliography{robust-software}

\end{document}
